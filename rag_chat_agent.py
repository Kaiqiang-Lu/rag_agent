import os
import tempfile
import streamlit as st
from typing import List, Optional
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.llms.base import LLM
from google import genai
from sentence_transformers import CrossEncoder


# é¡µé¢é…ç½®
st.set_page_config(page_title="RAG æ–‡æ¡£é—®ç­”æ™ºèƒ½ä½“", layout="wide")
st.title("ğŸ“„ æ–‡æ¡£é—®ç­”å°åŠ©æ‰‹")

# æ–‡ä»¶ä¸Šä¼ 
uploaded_files = st.sidebar.file_uploader(
    label="ğŸ“‚ ä¸Šä¼  txt æ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰", type=["txt"], accept_multiple_files=True
)
if not uploaded_files:
    st.info("è¯·å…ˆä¸Šä¼  txt æ–‡æ¡£ã€‚")
    st.stop()


# æ„å»º FAISS æ£€ç´¢å™¨
@st.cache_resource(ttl="1h")
def configure_retriever(uploaded_files):
    docs = []
    temp_dir = tempfile.TemporaryDirectory()

    for file in uploaded_files:
        temp_path = os.path.join(temp_dir.name, file.name)
        with open(temp_path, "wb") as f:
            f.write(file.getvalue())
        loader = TextLoader(temp_path, encoding="utf-8")
        docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-large")
    vectordb = FAISS.from_documents(splits, embeddings)
    return vectordb.as_retriever(search_kwargs={"k": 8}) 


retriever = configure_retriever(uploaded_files)


# Gemini æ¨¡å‹å°è£…
load_dotenv()
google_client = genai.Client()


class GeminiLLM(LLM):
    model: str = "gemini-2.5-flash"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = google_client.models.generate_content(
            model=self.model,
            contents=prompt
        )
        return response.text.strip()

    @property
    def _identifying_params(self) -> dict:
        return {"model": self.model}

    @property
    def _llm_type(self) -> str:
        return "google_gemini"


llm = GeminiLLM()


# é‡æ’æ¨¡å‹ 
@st.cache_resource()
def load_reranker():
    model_name = "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"
    return CrossEncoder(model_name)

reranker = load_reranker()


def rerank_docs(query: str, retrieved_docs: List[str], top_k: int = 3) -> List[str]:
    pairs = [(query, doc) for doc in retrieved_docs]
    scores = reranker.predict(pairs)
    scored = list(zip(retrieved_docs, scores))
    scored.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in scored[:top_k]]


# èŠå¤©è®°å¿†
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


# Prompt æ¨¡æ¿
qa_prompt = PromptTemplate.from_template("{query}")


# æ„å»º LLM é“¾
qa_chain = LLMChain(llm=llm, prompt=qa_prompt)  



# é¡µé¢äº¤äº’é€»è¾‘
if "messages" not in st.session_state or st.sidebar.button("ğŸ§¹ æ¸…é™¤èŠå¤©è®°å½•"):
    st.session_state["messages"] = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ–‡æ¡£é—®ç­”æ™ºèƒ½å°åŠ©æ‰‹ã€‚"}
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
st.markdown("""
    <style>
        /* ä»…å½“ Streamlit äº§ç”Ÿæµ…ç°å¤åˆ¶æ—¶ï¼Œéšè—å…¶é‡å¤å…ƒç´  */
        .stChatFloatingInput + div .stChatMessage:last-child {
            display: none !important;
        }
    </style>
""", unsafe_allow_html=True)
user_query = st.chat_input(placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})
    st.chat_message("user").write(user_query)

    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ£€ç´¢æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”..."):
            try:
                # æ£€ç´¢
                retrieved_docs = retriever.get_relevant_documents(user_query)
                retrieved_texts = [doc.page_content for doc in retrieved_docs]

                # é‡æ’
                reranked_texts = rerank_docs(user_query, retrieved_texts, top_k=3)

                # ç”Ÿæˆ
                context = "\n\n".join(reranked_texts)
                # ä» memory å–å‡ºå†å²èŠå¤©è®°å½•
                past_msgs = memory.load_memory_variables({}).get("chat_history", [])
                history_text = ""
                if past_msgs:
                    for msg in past_msgs:
                        role = "ç”¨æˆ·" if msg.type == "human" else "åŠ©æ‰‹"
                        history_text += f"{role}: {msg.content}\n"

                # æ„é€ å•ä¸€çš„ query
                composed_query = f"""
                ä½ æ˜¯ä¸€ä½ä¸­æ–‡çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£ä¿¡æ¯å›ç­”æœ€åçš„é—®é¢˜ï¼Œè¦æ±‚è‡ªç„¶ã€å‡†ç¡®ã€é€»è¾‘æ¸…æ™°ã€‚

                ã€æ–‡æ¡£å†…å®¹ã€‘
                {context}

                ã€å†å²å¯¹è¯ã€‘
                {history_text}

                ã€å½“å‰é—®é¢˜ã€‘
                {user_query}

                è‹¥æ— æ˜ç¡®ç­”æ¡ˆï¼Œè¯·å›ç­”ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰åœ¨ä¸Šä¼ æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚â€ã€‚
                """

                response = qa_chain.invoke({"query": composed_query})
                answer = response["text"]

                # æ‰‹åŠ¨å†™å…¥memory
                memory.save_context({"input": user_query}, {"output": answer})

            except Exception as e:
                answer = f"âŒ è°ƒç”¨å‡ºé”™ï¼š{e}"

            st.session_state.messages.append({"role": "assistant", "content": answer})
            st.write(answer)
