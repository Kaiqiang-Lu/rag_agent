import os
import tempfile
import streamlit as st
from typing import List, Optional, Generator
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.llms.base import LLM
from google import genai
from sentence_transformers import CrossEncoder


# é¡µé¢é…ç½®
st.set_page_config(page_title="RAG æ–‡æ¡£é—®ç­”æ™ºèƒ½ä½“", layout="wide")
st.title("ğŸ“„ æ–‡æ¡£é—®ç­”å°åŠ©æ‰‹")

# æ–‡ä»¶ä¸Šä¼ 
uploaded_files = st.sidebar.file_uploader(
    label="ğŸ“‚ ä¸Šä¼  txt æ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰", type=["txt"], accept_multiple_files=True
)
if not uploaded_files:
    st.info("è¯·å…ˆä¸Šä¼  txt æ–‡æ¡£ã€‚")
    st.stop()


# æ„å»º FAISS æ£€ç´¢å™¨
@st.cache_resource(ttl="1h")
def configure_retriever(uploaded_files):
    docs = []
    temp_dir = tempfile.TemporaryDirectory()

    for file in uploaded_files:
        temp_path = os.path.join(temp_dir.name, file.name)
        with open(temp_path, "wb") as f:
            f.write(file.getvalue())
        loader = TextLoader(temp_path, encoding="utf-8")
        docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-large")
    vectordb = FAISS.from_documents(splits, embeddings)
    return vectordb.as_retriever(search_kwargs={"k": 8})


retriever = configure_retriever(uploaded_files)


# Gemini æ¨¡å‹å°è£…
load_dotenv()
google_client = genai.Client()


class GeminiLLM(LLM):
    model: str = "gemini-2.5-flash"

    # Streaming è¾“å‡º
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        try:
            stream = google_client.models.generate_content_stream(
                model=self.model,
                contents=prompt
            )
        except:
            response = google_client.models.generate_content(
                model=self.model,
                contents=prompt
            )
            return response.text.strip()

        streamed_text = ""
        for chunk in stream:
            if hasattr(chunk, "text") and chunk.text:
                streamed_text += chunk.text
        return streamed_text

    @property
    def _identifying_params(self) -> dict:
        return {"model": self.model}

    @property
    def _llm_type(self) -> str:
        return "google_gemini"


llm = GeminiLLM()


# é‡æ’æ¨¡å‹
@st.cache_resource()
def load_reranker():
    model_name = "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"
    return CrossEncoder(model_name)


reranker = load_reranker()


def rerank_docs(query: str, retrieved_docs: List[str], top_k: int = 3) -> List[str]:
    pairs = [(query, doc) for doc in retrieved_docs]
    scores = reranker.predict(pairs)
    scored = list(zip(retrieved_docs, scores))
    scored.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in scored[:top_k]]


# èŠå¤©è®°å¿†
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Prompt æ¨¡æ¿
qa_prompt = PromptTemplate.from_template("{query}")

# æ„å»º LLM é“¾
qa_chain = LLMChain(llm=llm, prompt=qa_prompt)


# åˆ¤æ–­æ˜¯å¦éœ€è¦çŸ¥è¯†åº“æ£€ç´¢
def should_retrieve(user_query: str) -> bool:
    decision_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢åˆ¤æ–­åŠ©æ‰‹ã€‚åˆ¤æ–­ä¸‹é¢çš„é—®é¢˜æ˜¯å¦å¯èƒ½éœ€è¦æŸ¥è¯¢ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹æ‰èƒ½å¾—åˆ°æ›´å‡†ç¡®çš„å›ç­”ã€‚

è‹¥é—®é¢˜åŒ…å«ï¼š
- æ–‡æ¡£ä¸»é¢˜ã€å†…å®¹ã€å…³é”®è¯ã€å¥å­å«ä¹‰ã€å†…å®¹è§£é‡Š
- éœ€è¦å¼•ç”¨æ–‡æ¡£å†…å®¹å›ç­”
â†’ å›ç­” yes

è‹¥é—®é¢˜æ˜æ˜¾æ˜¯é—²èŠï¼Œå¦‚ï¼š
- ä½ å¥½
- ä½ æ˜¯è°
- ä½ æ˜¯ AI å—
â†’ å›ç­” no

æ³¨æ„ï¼šå¦‚æœä¸èƒ½ç¡®å®šï¼Œè¯·å›ç­” yesã€‚
åªå›ç­” yes æˆ– noã€‚

é—®é¢˜ï¼š{user_query}
"""
    try:
        resp = google_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=decision_prompt
        ).text.strip().lower()
        return "yes" in resp
    except:
        return True


# é¡µé¢äº¤äº’é€»è¾‘
if "messages" not in st.session_state or st.sidebar.button("ğŸ§¹ æ¸…é™¤èŠå¤©è®°å½•"):
    st.session_state["messages"] = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ–‡æ¡£é—®ç­”æ™ºèƒ½å°åŠ©æ‰‹ã€‚"}
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

st.markdown("""
    <style>
        .stChatFloatingInput + div .stChatMessage:last-child {
            display: none !important;
        }
    </style>
""", unsafe_allow_html=True)

user_query = st.chat_input(placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})
    st.chat_message("user").write(user_query)

    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            try:
                # åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
                need_retrieve = should_retrieve(user_query)

                retrieved_texts = []
                if need_retrieve:
                    retrieved_docs = retriever.get_relevant_documents(user_query)
                    retrieved_texts = [doc.page_content for doc in retrieved_docs]
                    reranked_texts = rerank_docs(user_query, retrieved_texts, top_k=3)
                    context = "\n\n".join(reranked_texts)
                else:
                    context = ""

                # å†å²å¯¹è¯
                past_msgs = memory.load_memory_variables({}).get("chat_history", [])
                history_text = ""
                if past_msgs:
                    for msg in past_msgs:
                        role = "ç”¨æˆ·" if msg.type == "human" else "åŠ©æ‰‹"
                        history_text += f"{role}: {msg.content}\n"

                # æ„é€  Prompt
                composed_query = f"""
ä½ æ˜¯ä¸€ä½ä¸­æ–‡æ–‡æ¡£çŸ¥è¯†å°åŠ©æ‰‹ï¼Œä½ çš„å›ç­”åº”å½“ç®€æ´ã€å‡†ç¡®ã€å‹å¥½ã€‚

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€å†å²å¯¹è¯ã€‘
{history_text}

ã€å½“å‰é—®é¢˜ã€‘
{user_query}

è‹¥æ–‡æ¡£ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥æŒ‰å¸¸è¯†å›ç­”ã€‚
"""

                # æµå¼è¾“å‡º
                final_prompt = qa_prompt.format(query=composed_query)

                placeholder = st.empty()
                full_text = ""

                stream = google_client.models.generate_content_stream(
                    model=llm.model,
                    contents=final_prompt
                )

                for chunk in stream:
                    if hasattr(chunk, "text") and chunk.text:
                        full_text += chunk.text
                        placeholder.write(full_text)

                answer = full_text
                memory.save_context({"input": user_query}, {"output": answer})

            except Exception as e:
                answer = f"âŒ è°ƒç”¨å‡ºé”™ï¼š{e}"

            st.session_state.messages.append({"role": "assistant", "content": answer})
